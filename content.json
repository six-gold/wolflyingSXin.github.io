[{"title":"Debug Spark","date":"2017-10-13T03:35:21.000Z","path":"2017/10/13/Debug-Spark/","text":"Remote Debug Spark Application In Local ModelCreate Remote Debug Configuration In Idea Run &gt;&gt; Edit Configurations &gt;&gt; Remote &gt;&gt; Setting &gt;&gt; Debug mode: Attach &gt;&gt; Modify Host and Port Set System EnvironmentDebug in Local Modeexport SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=wolflying:5005wolflying same as the Idea remote debug Host, 5005 same as the Idea remote debug Port. Debug in Standalone Mode debug driver./bin/spark-submit –class org.apache.spark.examples.JavaWordCount –master spark://wolflying:6066 –deploy-mode cluster -v –conf spark.rpc.askTimeout=600s –conf spark.driver.extraJavaOptions=”-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=wolflying:5005” examples/target/original-spark-examples_2.11-2.1.1.jar file:///home/wolflying/Documents/docs/index.html–conf spark.driver.extraJavaOptions=”-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=wolflying:5005” debug executor./bin/spark-submit –class org.apache.spark.examples.JavaWordCount –master spark://wolflying:6066 –deploy-mode cluster -v –conf spark.rpc.askTimeout=600s –conf spark.executor.extraJavaOptions=”-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=wolflying:5005” examples/target/original-spark-examples_2.11-2.1.1.jar file:///home/wolflying/Documents/docs/index.html–conf spark.executor.extraJavaOptions=”-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=wolflying:5005” Submit Spark Job./bin/spark-submit –class org.apache.spark.examples.JavaWordCount –master local examples/target/original-spark-examples_2.11-2.1.1.jar file:///home/wolflying/Documents/docs/index.html OthersBuild Spark Using Maven./build/mvn package./build/mvn -Dscala-2.1.1 -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -Phive -Phive-thriftserver -DskipTests clean package Build Spark Using Sbt./build/sbt package./build/sbt -Dscala-2.1.1 -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -Phive -Phive-thriftserver -DskipTests clean package spark official build docsspark debug environment deployment","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"How Spark run an application","date":"2017-10-13T03:35:13.000Z","path":"2017/10/13/How-Spark-run-an-application/","text":"Standalone Cluster Mode Submitting application to cluster SparkSubmitArguments -&gt; Parses and encapsulates(封装) arguments from the spark-submit script. prepareSubmitEnvironment -&gt; Prepare the environment for submitting an application. childMainClass: org.apache.spark.deploy.rest.RestSubmissionClient. invoke childMainClass.main RestSubmissionClient.constructSubmitRequest -&gt; Construct a message that captures the specified parameters for submitting an application. RestSubmissionClient.createSubmission -&gt; Submit an application specified by the parameters in the provided request. Receive and Process the Submitting Application Request (REST API) StandaloneSubmitRequestServlet.handleSubmit -&gt; Handle the submit request and construct an appropriate response to return to the client. StandaloneSubmitRequestServlet.buildDriverDescription -&gt; Build a driver description from the fields specified in the submit request. DeployMessages.RequestSubmitDriver -&gt; send RequestSubmitDriver message to Spark Master Master.receiveAndReply -&gt; process received message Master.createDriver -&gt; create DriverInfo object Master.schedule -&gt; Schedule the currently available resources among waiting apps. Master.launchDriver -&gt; Send LaunchDriver message to Worker Worker new DriverRunner -&gt; Worker received the LaunchDriver message and create a DriverRunner object. DriverRunner: Manages the execution of one driver. DriverRunner.runDriver -&gt; Launch Driver Command DriverWrapper.main -&gt; launching driver programs(Standalone cluster mode only) invoke Application main method SparkSession.getOrCreate -&gt; get an existing SparkSession or creates a new one. process application. Someting important when Create a SparkContext (Standalone Cluster Mode)SparkContext try catch block SparkContext.createTaskScheduler -&gt; Create a task schedule based on a given master URL. new TaskSchedulerImp -&gt; new StandaloneSchedulerBackend -&gt; A SchedulerBackend implementation for Spark’s standalone cluster manager. new DAGScheduler -&gt; Create a DAG scheduler _taskScheduler.start -&gt; start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler’s constructor. new ApplicationDescription new StandaloneAppClient -&gt; Interface allowing applications to speak with a Spark standalone cluster manager. StandaloneAppClient.start -&gt; Just launch an AppClient rpcEndpoint Someting important when process ActionsRDD actions run jobs using runJob method. runJob method run a function on a given set of partitions in an RDD and pass the results to the given handler function. getCallsite -&gt; capture the current user callsite and return a formatted version for printing. clean -&gt; Clean a closure to make it ready to serialized and sent to tasks (removes unreferenced in outer’s, updates REPL variables) dagScheduler.runJob -&gt; Run an action job on the given RDD and pass all the results to the resultHandler function as they arrive. DAGScheduler runJob method DAGScheduler.submitJob -&gt; submit an action job to scheduler. post JobSubmitted event -&gt; Types of events that can be handled by the DAGScheduler. DAGScheduler.handleJobSubmitted -&gt; process JobSubmitted event. DAGScheduler.createResultStage -&gt; Create a ResultStage associated with the provided jobID. DAGScheduler.getOrCreateParentStages -&gt; Get or create the list of parent stages for a given RDD. DAGScheduler.getShuffleDependencies -&gt; Returns shuffle dependencies that are immediate parents of the given RDD. DAGScheduler.getOrCreateShuffleMapStage -&gt; Gets a shuffle map stage if one exists in shuffleIdToMapStage. Otherwise, if the shuffle map stage doesn’t already exists, this method will create the shuffle map stage in addition to any missing ancestor shuffler map stages. new ResultStage -&gt; ResultStage apply a function on some partitions of an RDD to compute the result of an action. new ActiveJob -&gt; A running job in the DAGScheduler. DAGScheduler.submitStage -&gt; Submits stage, but first recursively submits any missing parents. DAGScheduler.submitMissingTask -&gt; Called when stage’s parents are available and we can now do its task. OutputCommmitCoordinator.stageStart -&gt; Called by the DAGScheduler when a stage starts. Stage.makeNewStageAttempt -&gt; Creates a new attempt for this stage by creating a new StageInfo with a new attempt ID. SparkContext.broadcast -&gt; Broadcasted binary for the task, used to dispatch tasks to executors. new ShuffleMapTask/ResultTask -&gt; create Task object. TaskSchedulerImp.submitTasks -&gt; TaskSchedulerImp.createTaskSetManager -&gt; Create a TaskSetManager. Schedules the tasks within a single TaskSet. CoarseGrainedSchedulerBackend(SchedulerBackend).reviveOffers driverEndpoint send ReviveOffers message CoarseGrainedSchedulerBackend.makeOffers -&gt; Make fake resource offers on all executors. TaskSchedulerImp.resourceOffers -&gt; Called by cluster manager to offer resources on slaves. CoarseGrainedSchedulerBackend.launchTasks -&gt; Launch tasks returned by a set of resource offers. executorEndpoint send LaunchTask message Executor.launchTask new TaskRunner -&gt; TaskRunner extends Runnable interface. This create a new thread, add it to Executor worker thread pool TaskRunner.run prepare TaskContext of the Task ShuffleMapTask/ResultTask.runTask","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]}]